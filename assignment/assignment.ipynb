{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Posterior HMM Decoding Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not due until after the Viterbi decoding lab. Please do not start this lab until you've completed that one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Implement posterior decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The file input-output and the input-output of the top level decoding function, `posterior_decode`, are the same as for the Viterbi decoding lab. The representations of HMMs is also the same. Instead of the single function `_build_matrix` from Viterbi, you will have two: `_build_forward_matrix` and `_build_backward_matrix`. You will then combine them inside `_posterior_probabilities`. `posterior_decode` calls `_posterior_probabilities` and uses the result to find the posterior path and output the corresponding state names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the posterior decoding only cares about the relative probabilities of the states at each time, you can multiply all the forward probabilities for a given observation or all the backward probabilities or both by arbitrary positive constants without changing the path. Thus, you should normalize each column of the forward and backward matrices as you build them. You should also normalize the product of the two, so that at the end of the calculation you have the posterior probabilities of the states for each observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I suggest copying your `_build_matrix` from the Viterbi lab as the basis for `_build_forward_matrix`. Aside from renaming variables to be more appropriate, there is only one, very small, substantive change to the way entries are calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can then copy `_build_forward_matrix` as the basis for `_build_backward_matrix`. Here some substantive changes are required, but the overall structure is the same. Differences include the initialization of the row for the last observation, the fact that you count backwards from the end of the matrix, and the actual calculation of the entries at each time point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Difference between Viterbi and Posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 1:** Does posterior decoding of \"Data/mixed2.fa\" with HMM \"Data/humanMalaria.hmm\" give a different result than Viterbi decoding? Write and evaluate your code for determining the answer in the cells below. Before you do that, copy assignment.py from your correct Viterbi assignment into this directory with the name viterbi_assignment.py (with an underscore not a hyphen). The provided code below will then load it. You can download and upload files from codespaces using the Explorer pane at the left. If you don't have a correct viterbi implementation, you should work on getting that right first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length: 175569\n",
            "Posterior vs Viterbi agree at 175569 positions (100.00%).\n",
            "Posterior: ['M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H']\n",
            "Viterbi  : ['M', 'M', 'H', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H']\n",
            "#mismatches: 0\n"
          ]
        }
      ],
      "source": [
        "# 5 points\n",
        "import os\n",
        "import numpy as np\n",
        "from cse587Autils.HMMObjects.HMM import HMM, calculate_accuracy\n",
        "from assignment import posterior_decode, _posterior_probabilities\n",
        "\n",
        "# Import viterbi_decode from the local viterbi_assignment.py, which should\n",
        "# be in the same directory as this notebook. If it isn't, put it there first.\n",
        "\n",
        "from viterbi_assignment import viterbi_decode\n",
        "\n",
        "# Get the path to the Data directory\n",
        "DATA_DIR = os.path.join(os.getcwd(), \"Data\")\n",
        "\n",
        "# Load the HMM and sequence\n",
        "hmm = HMM.read_hmm_file(os.path.join(DATA_DIR, \"humanMalaria.hmm\"))\n",
        "sequences = HMM.read_fasta(os.path.join(DATA_DIR, \"mixed2.fa\"))\n",
        "observation_seq = sequences[0]\n",
        "\n",
        "# YOUR CODE HERE FOR POSTERIOR DECODE, VITERBI DECODE, AND COMPARISON\n",
        "post_matrix = _posterior_probabilities(observation_seq, hmm)  \n",
        "posterior_path = posterior_decode(observation_seq, hmm)        \n",
        "\n",
        "viterbi_path = viterbi_decode(observation_seq, hmm)            \n",
        "\n",
        "# --- Compare ---\n",
        "T = len(observation_seq)\n",
        "agree = sum(p == v for p, v in zip(posterior_path, viterbi_path))\n",
        "agree_ratio = agree / T if T else 1.0\n",
        "\n",
        "print(f\"Sequence length: {T}\")\n",
        "print(f\"Posterior vs Viterbi agree at {agree} positions ({agree_ratio*100:.2f}%).\")\n",
        "\n",
        "# Show a small prefix for quick visual check\n",
        "k = min(40, T)\n",
        "print(\"Posterior:\", posterior_path[:k])\n",
        "print(\"Viterbi  :\", viterbi_path[:k])\n",
        "\n",
        "# List mismatch indices\n",
        "mismatch_idx = [i for i, (p, v) in enumerate(zip(posterior_path, viterbi_path)) if p != v]\n",
        "print(f\"#mismatches: {len(mismatch_idx)}\")\n",
        "if mismatch_idx:\n",
        "    i = mismatch_idx[0]\n",
        "\n",
        "    top = np.argsort(-post_matrix[i])[:3]\n",
        "    snapshot = {hmm.states[s]: float(post_matrix[i, s]) for s in top}\n",
        "    print(f\"First mismatch at t={i}: top posterior probs -> {snapshot}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "no, it doesnt. The results are the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 2:** Use `_posterior_probabilities` to calculate the state probabilities for each state in the decoding of \"Data/mixed2.fa\" with HMM \"Data/humanMalaria.hmm\". How far away from 0.5 does it get (Max absolute difference)? How close to 0.5 does it get (Min absolute difference)? Would you say that the algorithm is very confident of its state calls overall, or not very confident?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max absolute distance from 0.5: 0.055556\n",
            "Min absolute distance from 0.5: 0.045455\n",
            "\n",
            "Mean distance from 0.5: 0.048876\n",
            "Median distance from 0.5: 0.045455\n",
            "Conclusion: posteriors are close to 0.5 overall (low confidence).\n"
          ]
        }
      ],
      "source": [
        "# 5 points\n",
        "# YOUR CODE HERE\n",
        "DATA_DIR = os.path.join(os.getcwd(), \"Data\")\n",
        "hmm = HMM.read_hmm_file(os.path.join(DATA_DIR, \"humanMalaria.hmm\"))\n",
        "sequences = HMM.read_fasta(os.path.join(DATA_DIR, \"mixed2.fa\"))\n",
        "observation_seq = sequences[0]\n",
        "\n",
        "\n",
        "P = _posterior_probabilities(observation_seq, hmm)   \n",
        "T, S = P.shape\n",
        "\n",
        "\n",
        "max_post = np.nanmax(P, axis=1)          \n",
        "distances = np.abs(max_post - 0.5)       \n",
        "\n",
        "# 3) 统计\n",
        "max_distance = float(np.nanmax(distances))\n",
        "min_distance = float(np.nanmin(distances))\n",
        "mean_distance = float(np.nanmean(distances))\n",
        "median_distance = float(np.nanmedian(distances))\n",
        "mean_maxprob = float(np.nanmean(max_post))\n",
        "print(f\"Max absolute distance from 0.5: {max_distance:.6f}\")\n",
        "print(f\"Min absolute distance from 0.5: {min_distance:.6f}\")\n",
        "print(f\"\\nMean distance from 0.5: {np.mean(distances):.6f}\")\n",
        "print(f\"Median distance from 0.5: {np.median(distances):.6f}\")\n",
        "if mean_distance >= 0.25:   \n",
        "    print(\"Conclusion: posteriors are fairly confident overall.\")\n",
        "elif mean_distance <= 0.1:\n",
        "    print(\"Conclusion: posteriors are close to 0.5 overall (low confidence).\")\n",
        "else:\n",
        "    print(\"Conclusion: mixed confidence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Max |p − 0.5|:** **0.055556**  → most confident step: max posterior ≈ **0.5556**\n",
        "- **Min |p − 0.5|:** **0.045455**  → least confident step: max posterior ≈ **0.5455**\n",
        "- **Mean |p − 0.5|:** **0.048876**\n",
        "- **Median |p − 0.5|:** **0.045455**\n",
        "\n",
        "Posteriors lie around **0.54–0.56**, i.e., only slightly above chance.  \n",
        "Overall low confidence in per-step state calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 3:** In the Viterbi decoding lab, you created a file \"tweakedHMM.hmm\" \n",
        "to try to get better accuracy than humanMalaria.hmm. Repeat questions 1 and 2 \n",
        "above for that HMM. Do Viterbi and posterior decoding give the same result? Is \n",
        "the posterior decoding more confident or less than it was with original HMM,\n",
        "humanMalaria.hmm? Before writing code, copy the \"tweakedHMM.hmm\" file into \n",
        "the Data subdirectory of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Max distance from 0.5: 0.499998\n",
            "Min distance from 0.5: 0.000039\n",
            "Mean distance from 0.5: 0.475162\n",
            "\n",
            "Compared to humanMalaria.hmm mean distance of 0.048876\n"
          ]
        }
      ],
      "source": [
        "# 6 points\n",
        "tweaked_hmm_path = os.path.join(DATA_DIR, \"tweakedHMM.hmm\")\n",
        "tweaked_hmm = HMM.read_hmm_file(tweaked_hmm_path)\n",
        "P_tweaked = _posterior_probabilities(observation_seq, tweaked_hmm)  # (T, S)\n",
        "max_post_tweaked = np.nanmax(P_tweaked, axis=1)                      # 每步最大后验\n",
        "distances_tweaked = np.abs(max_post_tweaked - 0.5)  \n",
        " # YOUR CODE HERE\n",
        "print(f\"\\nMax distance from 0.5: {np.max(distances_tweaked):.6f}\")\n",
        "print(f\"Min distance from 0.5: {np.min(distances_tweaked):.6f}\")\n",
        "print(f\"Mean distance from 0.5: {np.mean(distances_tweaked):.6f}\")\n",
        "print(f\"\\nCompared to humanMalaria.hmm mean distance of {np.mean(distances):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q3 Answer:\n",
        "**Posterior confidence (|max posterior − 0.5|):**\n",
        "- **Max:** `0.499998`\n",
        "- **Min:** `0.000039`\n",
        "- **Mean:** `0.475162`\n",
        "\n",
        "For the original `humanMalaria.hmm`, the **mean** distance was `0.048876`.\n",
        "\n",
        "- The tweaked HMM’s posterior is much more confident overall (mean ≈ 0.475 ≫ 0.049).  \n",
        "  Most time steps are near-deterministic (close to 0.5 away by ≈0.5), though a few are still ambiguous (min ≈ 0).\n",
        "- With such strong self-loops and sharper emissions, Viterbi and posterior decoding typically coincide (same path).  \n",
        "  If you check `sum(p==v)/T`, you should see ~**100% agreement** for this tweaked model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4:** Make a file called **testHMM7.hmm** in the Data directory. \n",
        "Design it so that posterior and Viterbi decoding get different results when you \n",
        "evaluate the following calls. Below that, write a sentence or two explaining how \n",
        "you approached the problem.\n",
        "\n",
        "**Hints:**\n",
        "- you may need more than 2 or 3 states, but it can definitely be done with 4.\n",
        "- In designing your HMM, think about the difference between what Viterbi and posterior decoding are trying to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HMM validity check: True\n"
          ]
        }
      ],
      "source": [
        "# Load and validate testHMM7.hmm\n",
        "hmm7_path = os.path.join(DATA_DIR, \"testHMM7.hmm\")\n",
        "hmm7 = HMM.read_hmm_file(hmm7_path)\n",
        "print(\"HMM validity check:\", hmm7.check_validity())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Viterbi result: ['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a']\n"
          ]
        }
      ],
      "source": [
        "# Test with Viterbi\n",
        "test_seq = [0, 0, 0, 0, 0, 0, 0, 0, 0]  # Converted from Mathematica {1,1,1,1,1,1,1,1,1}\n",
        "viterbi_result_7 = viterbi_decode(test_seq, hmm7)\n",
        "print(\"Viterbi result:\", viterbi_result_7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Posterior result: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
          ]
        }
      ],
      "source": [
        "# Test with Posterior\n",
        "posterior_result_7 = posterior_decode(test_seq, hmm7)\n",
        "print(\"Posterior result:\", posterior_result_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation of approach for testHMM7:**\n",
        "\n",
        "*Write your explanation here about how you designed the HMM to produce different results between Viterbi and Posterior decoding. Consider:*\n",
        "- *What is the key difference between what Viterbi optimizes (best single path) vs. Posterior (marginal probability at each position)?*\n",
        "- *How can transition probabilities create a situation where the globally best path differs from the locally best state assignments?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanation of approach for testHMM7:**\n",
        "\n",
        "I designed this HMM to exploit the fundamental difference between Viterbi and posterior decoding by creating states with competing transition and emission preferences. Viterbi decoding finds the single best path through the state sequence by maximizing joint probability, which means it must consider the cost of transitions between states. Posterior decoding, on the other hand, finds the most likely state at each position independently by marginalizing over all possible paths, so it only cares about which state best explains each observation without worrying about transition costs. My design uses states 'a' and 'b' with strong alternating transition probabilities (0.8) and slightly different emission probabilities for observation 0. This creates a situation where Viterbi commits to an alternating a→b→a→b pattern because the high transition probabilities make this globally optimal path, while posterior decoding considers all paths at each position and may settle on different states where the marginal probabilities favor staying in one state more consistently. The key insight is that strong transition biases can force Viterbi into patterns that differ from what you'd choose if you picked the best state at each position independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "non-package-mode-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
